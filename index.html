<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Understanding Generative Flow Networks (GFlowNets) – A User perspective</title>
  <!-- Load Dagre first -->
<script src="https://unpkg.com/dagre/dist/dagre.min.js"></script>
<!-- KaTeX CSS -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css"
      integrity="sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js"
        integrity="sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js"
        integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

        <script src="https://d3js.org/d3.v7.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/dagre@0.8.5/dist/dagre.min.js"></script>


<style>
  nav {
  position: fixed;
  top: 0;
  left: 0;
  width: 250px;
  height: 100vh;
  background: rgba(0,0,0,0.9);
  transform: translateX(-270px);
  transition: transform 0.3s ease;
  padding: 60px 20px;
  overflow-y: auto;
  z-index: 9999;
}
nav.show {
  transform: translateX(0);
}
.nav-menu {
  list-style: none;
}
.nav-menu li {
  margin-bottom: 15px;
}
.nav-menu a {
  color: #eee;
  text-decoration: none;
  font-weight: 600;
  font-size: 1rem;
}
.nav-menu a:hover {
  text-decoration: underline;
}
  details > summary + p + ul {
  margin-left: 2em;  /* or whatever spacing you prefer */
}

/* Optional: Some extra styling for math blocks */
.katex-display {
margin: 1em 0;
text-align: center;
}
code {
background-color: #f3f3f3;
padding: 2px 4px;
border-radius: 4px;
font-size: 90%;
}
details > summary {
cursor: pointer;
font-weight: bold;
}

* {
  margin: 0;
  padding: 0;
  box-sizing: border-box;
}
html, body {
  width: 100%;
  height: 100%;
  background: #111;
  color: #eee;
  font-family: "Helvetica Neue", Arial, sans-serif;
  line-height: 1.6;
  overflow-x: hidden;
}
/* (Line 20) Headings & Text */
h1, h2, h3, h4, h5, h6 {
  font-weight: 700;
  margin-bottom: 10px;
  color: #00bfff;
}
h1 {
  font-size: 2rem;
  margin-top: 80px;
  text-align: center;
  color: #00bfff;
}
h2 {
  margin-top: 40px;
  color: #fefefe;
}
p, li {
  margin-bottom: 10px;
}
a {
  color: #00bfff;
  text-decoration: none;
}
a:hover {
  text-decoration: underline;
}
em {
  font-style: italic;
}
code {
  background: rgba(255,255,255,0.07);
  padding: 3px 5px;
  border-radius: 4px;
  font-family: "Courier New", monospace;
}
pre {
  background: rgba(255,255,255,0.1);
  padding: 10px;
  border-radius: 6px;
  overflow-x: auto;
  margin: 15px 0;
}
/* (Line 50) Layout: main container */
main {
  max-width: 1200px;
  margin: 0 auto;
  padding: 60px 20px;
  position: relative;
  z-index: 1; /* Over the particle background */
}
/* (Line 60) Sections styling */
section {
  margin-bottom: 100px;
  background: rgba(255,255,255,0.02);
  border-radius: 6px;
  padding: 20px;
  transition: all 0.4s ease;
}
section.active {
  background: rgba(255,255,255,0.06);
  transform: translateY(-2px);
  box-shadow: 0 2px 8px rgba(0,0,0,0.5);
}
.section-content {
  margin: 0 10px;
}

/* (Line 75) Particle Background Container */
#particles-js {
  position: fixed;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  z-index: -10; /* behind everything */
}

/* (Line 80) Header & Burger Menu */
header {
  position: fixed;
  top: 10px;
  left: 10px;
  z-index: 9999;
}
.burger {
  width: 30px;
  height: 24px;
  display: flex;
  flex-direction: column;
  justify-content: space-between;
  cursor: pointer;
}
.burger span {
  display: block;
  height: 4px;
  background: #fff;
  border-radius: 2px;
}

/* (Line 100) Navigation (sidebar) */
nav {
  position: fixed;
  top: 0;
  left: 0;
  width: 250px;
  height: 100vh;
  background: rgba(0,0,0,0.9);
  transform: translateX(-270px);
  transition: transform 0.3s ease;
  padding: 60px 20px;
  overflow-y: auto;
  z-index: 9999;
}
nav.show {
  transform: translateX(0);
}
.nav-menu {
  list-style: none;
}
.nav-menu li {
  margin-bottom: 15px;
}
.nav-menu a {
  color: #eee;
  text-decoration: none;
  font-weight: 600;
  font-size: 1rem;
}
.nav-menu a:hover {
  text-decoration: underline;
}

/* (Line 125) The big page title H1 in the center */
h1 {
  margin-top: 120px;
  color: #00bfff;
  text-align: center;
  font-size: 2.4rem;
}

/* (Line 130) Sticky Arrow */
.sticky-arrow {
  position: fixed;
  bottom: 20px;
  right: 20px;
  background: #333;
  color: #fff;
  padding: 12px 15px;
  border-radius: 50%;
  cursor: pointer;
  font-size: 24px;
  z-index: 9999;
  text-align: center;
  line-height: 1;
  transition: background 0.3s;
}
.sticky-arrow:hover {
  background: #444;
}

/* (Line 145) Tetris container */
#tetris-game .container {
  display: flex;
  flex-wrap: wrap;
  gap: 24px;
  align-items: flex-start;
  justify-content: center;
}
#tetris-game .board {
  flex: 0 0 auto;
}
#tetris-game .sidebar {
  flex: 0 0 auto;
  min-width: 200px;
  max-width: 280px;
  background: rgba(0,0,0,0.3);
  padding: 15px;
  border-radius: 6px;
}
#tetrisCanvas {
  background: #222;
  border: 2px solid #444;
  display: block;
}

/* (Line 165) Candidate moves list */
#candidateList {
  margin-top: 10px;
}
.candidate {
  background: rgba(255,255,255,0.08);
  border-radius: 5px;
  padding: 8px;
  margin: 6px 0;
  cursor: pointer;
  transition: background 0.2s;
}
.candidate:hover {
  background: rgba(255,255,255,0.15);
}
.controls button {
  margin-top: 10px;
  padding: 10px 16px;
  background: #00bfff;
  border: none;
  border-radius: 4px;
  font-weight: 600;
  cursor: pointer;
  color: #111;
}
.controls button:hover {
  opacity: 0.85;
}

/* (Line 185) Collapsible details */
details {
  background: rgba(255,255,255,0.07);
  padding: 10px;
  border-radius: 5px;
  margin: 20px 0;
}
details summary {
  cursor: pointer;
  font-size: 1.05rem;
  font-weight: 600;
}
details summary::-webkit-details-marker {
  display: none;
}
details summary:before {
  content: '► ';
  color: #ffcc00;
}
details[open] summary:before {
  content: '▼ ';
  color: #ffcc00;
}

/* (Line 200) Reference lists */
.references {
  margin-top: 25px;
  background: rgba(255,255,255,0.04);
  padding: 10px;
  border-radius: 5px;
}
.references h3 {
  margin-bottom: 10px;
}
.references ul, .references ol {
  margin-left: 20px;
}
.references li {
  margin-bottom: 5px;
  line-height: 1.4;
}

#comparisonChart {
  display: flex;
  justify-content: center;   /* center them horizontally */
  gap: 20px;                 /* some spacing between the two mini DAGs */
}


/* (Line 230) Footer */
footer {
  text-align: center;
  padding: 20px 0;
  margin-top: 40px;
  background: rgba(255,255,255,0.02);
  border-top: 1px solid #444;
}



    /* Your existing dark-mode styles truncated for brevity... */
    html, body {
      background: #111;
      color: #eee;
      /* etc... */
    }

    /*
      Force KaTeX text to inherit the same color you use for
      the rest of the page. Otherwise KaTeX may default to black.
    */
    .katex, .katex-html, .katex-display {
      color: inherit !important;
    }

    /* Make display equations more readable against dark background */
    .katex-display {
      margin: 1em auto;
      text-align: center;
      font-size: 1.1rem; /* adjust if you want bigger/smaller math */
      line-height: 1.4;
    }

        /* =============== KaTeX Styling for Dark Mode =============== */
    /* Let KaTeX inherit the page's light text color */
    .katex, .katex-display, .katex-html {
      color: #f0f0f0 !important;
      /* 
         If you still see black text, your browser may be caching KaTeX font files 
         or there's another conflicting style. A hard refresh or clearing cache helps.
      */
    }

    /* Make inline math slightly larger than default */
    .katex {
      font-size: 1.06rem; 
      line-height: 1.5;
    }

    /* For display math (the $$ ... $$ kind): bigger, with a subtle background box */
    .katex-display {
      font-size: 1.15rem;
      line-height: 1.45;
      margin: 1.4em auto; 
      padding: 0.8em 1em;
      background: rgba(255,255,255,0.07);
      border-radius: 8px;
      box-shadow: 0 0 8px rgba(0,0,0,0.5);
      text-align: left; /* or center, if you prefer */
      max-width: 95%;
    }

    /*
       Improve fraction lines, roots, etc. on dark backgrounds
       KaTeX sets them as border-color, so let's override them
    */
    .katex .frac-line, 
    .katex .sqrt-line {
      border-color: #bbb !important;
    }

    /*
       Tweak the color for things like \tag, equation numbers, etc.
    */
    .katex-html .tag {
      background: #222 !important; 
      color: #ffcc00 !important; 
      border-radius: 4px;
      padding: 0 4px;
    }

    header {
  position: fixed;
  top: 10px;
  left: 10px;
  z-index: 99999; /* Higher than the nav */
}
.burger {
  width: 30px;
  height: 24px;
  display: flex;
  flex-direction: column;
  justify-content: space-between;
  cursor: pointer;
  /* optionally a background or small padding to ensure it's visible */
}

body {
  display: flex;
  flex-direction: column;
  min-height: 100vh;
  margin: 0;
  padding: 0;
}

main {
  flex: 1;
}

footer {
  flex-shrink: 0;
}

/* Completely hide the built-in details/summary triangle */
details > summary {
  list-style: none !important;         /* Firefox fallback */
}
details > summary::-webkit-details-marker {
  display: none !important;            /* WebKit */
}
details > summary::marker {
  content: none !important;            /* Modern browsers */
  display: none !important;
}

/* Then inject only your custom arrows */
details > summary::before {
  content: '► ' !important;
  color: #ffcc00 !important;
}
details[open] > summary::before {
  content: '▼ ' !important;
  color: #ffcc00 !important;
}

  </style>

</head>
<body>
<!-- (Line 250) Particle background container -->
<div id="particles-js"></div>

<!-- (Line 252) Header with burger menu button -->
<header>
  <div id="burger" class="burger" onclick="toggleNavMenu()">
    <span></span>
    <span></span>
    <span></span>
  </div>
</header>

<!-- (Line 260) Main page title -->
<h1>Understanding Generative Flow Networks (GFlowNets): A User perspective</h1>

<!-- (Line 262) Sidebar Navigation -->
<nav id="mainNav">
  <ul class="nav-menu">
    <li><a href="#introduction">1. Introduction: Generative Flow Networks</a></li>
    <li><a href="#tetris-demo">2. Tetris GFlowNet Demo</a></li>
    <li><a href="#motivation">3. Understanding GFlowNets</a></li>
    <li><a href="#theory">4. Core Concepts</a></li>
    <li><a href="#advanced-math">5. Advanced Math</a></li>
  </ul>
</nav>


<!-- (Line 275) Sticky arrow to scroll down or up -->
<div id="stickyArrow" class="sticky-arrow" onclick="scrollDown()">
  &#x2193;
</div>


<!-- (Line 280) MAIN CONTENT -->
<main>
  <section id="introduction">
    <div class="section-content">
      <h2>1. Introduction: Generative Flow Networks</h2>
      
      <p>
        A <strong>Generative Flow Network (GFlowNet)</strong> is a generative model 
        that learns to produce <em>multiple</em> high-quality solutions 
        (e.g., game moves, molecule designs) in proportion to their “reward.” 
        In other words, unlike many traditional AI methods such as reinforcement learning, it does not pursue just a single optimal path. Instead it maintains a 
        <em>diverse distribution</em> over near-optimal solutions. 
      </p>
  
      <p>
        Why does this matter? Because in real-world scenarios 
        like <strong>drug discovery</strong> or <strong>Tetris</strong>, you often want a <em>range</em> of 
        strong candidates. This way you can explore them all in parallel in the real world, if one fails another one might succeed. GFlowNets handle 
        this by learning to <em>flow</em> probability across all good options, which 
        leads to broader exploration and <strong>robustness</strong> when conditions change.
      </p>
  
      <!-- Comparison Chart: single-path vs. multi-path -->
      <div id="comparisonChart" style="width:100%; height:auto; min-height:400px;"></div>
      <script src="static/comparison.js"></script>
      <script>
        document.addEventListener("DOMContentLoaded", function() {
          initComparisonChart();
        });
      </script>
  
      <p>
        The comparison above highlights how “Traditional RL” may fixate on 
        one route, while a “GFlowNet” balances probability across many. 
        If you imagine each <em>path</em> as a way to build a solution, a GFlowNet 
        keeps multiple paths viable. This is especially helpful when you can not be sure 
        which path is truly the best.
      </p>
  
      <div class="references">
        <h3>Further Reading – Introduction</h3>
        <ul>
          <li>
            <strong>[1]</strong>
            Bengio, Y. et al. 
            <a href="https://arxiv.org/pdf/2106.04399.pdf" target="_blank">
              <em>Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation</em>
            </a>
            (NeurIPS 2021)
          </li>
          <li>
            <strong>[2]</strong>
            Bengio, Y. et al.
            <a href="https://arxiv.org/pdf/2111.09266.pdf" target="_blank">
              <em>GFlowNet Foundations</em>
            </a>
            (arXiv 2021)
          </li>
        </ul>
      </div>
    </div>
  </section>
  
  
  

  <!-- 
    =================================================================================
    SECTION 1: TETRIS DEMO
    We put the Tetris game first to wow the user. 
    The Tetris logic is embedded as "main.js" at the bottom, inlined for completeness.
    =================================================================================
  -->
  <section id="tetris-demo">
    <div class="section-content">
      <h2>2. Tetris GFlowNet Demo</h2>
      <p>
        Here, you can watch a <strong>GFlowNet</strong> in action, guiding an 
        interactive Tetris game. At each step, the system identifies multiple 
        promising moves—each assigned a <em>“flow”</em> value indicating how strongly 
        the GFlowNet favors it. Notably, it does not just stick to the 
        single most probable move, but it occasionally samples less-likely options, 
        ensuring exploration. Over time, it balances trying new strategies with 
        maximizing the reward (in this case, staying alive in Tetris for as long as possible). 
        This way the GFlowNet aims to match the real distribution of each move. 
      </p>
  
      <p>
        In the panel on the right, you will see the current top moves. 
        The GFlowNet automatically picks the first move of the list (green move), 
        but you are free to override it by clicking on any other move 
        to see how that might affect the game.
      </p>
    </div>
  
    <!-- Actual Tetris Canvas & Moves -->
    <section id="tetris-game">
      <div class="container">
        <div class="board">
          <canvas id="tetrisCanvas" width="300" height="600"></canvas>
        </div>
        <div class="sidebar">
          <h2>Candidate Moves</h2>
          <div id="candidateList"><!-- Populated by Tetris logic --></div>
          <div class="controls">
            <button id="resetBtn">Reset Game</button>
          </div>
        </div>
      </div>
    </section>

    <details>
      <summary>Implementation Details</summary>
      <p>
        The Tetris demonstration employs a pretrained neural GFlowNet policy. Within the <code>TetrisGame</code> environment, the agent receives partial line-clearing rewards (e.g., +300 for one line up to +3000 for four lines) supplemented by a fixed survival bonus. To promote structurally coherent play, the agent is penalized in proportion to the count of board holes, the maximum column height and board bumpiness following each piece placement.
      </p>
      <p>
        During training, each state–action pair is encoded into a 212-dimensional feature vector and passed through a multilayer perceptron with 512-unit hidden layers to predict unnormalized log-flow values. Trajectories are stored in a replay buffer and model parameters as well as the log-partition function <code>logZ</code> are jointly optimized using the trajectory-balance loss.
      </p>
      <p>
        Upon completion of training, the resulting model weights and <code>logZ</code> are serialized into JSON for use in the front-end. The JavaScript client loads this data to instantiate a <code>NeuralFlowNet</code>, which computes log-flow values for all terminal moves, applies a softmax to yield sampling probabilities and renders the top candidate moves within the Tetris interface.
      </p>
    </details>
    <details>
      <summary>Heuristic and State-Space Reduction</summary>
      <p>
        To mitigate the combinatorial explosion of terminal moves, the front end applies a <em>beam-blend heuristic</em>. For each candidate terminal move, two independent metrics are computed:
      </p>
      <ol>
          <strong>Flow score</strong>: The normalized log-flow output from the GFlowNet, reflecting the learned policy’s preference for the move.


          <strong>One-step lookahead score</strong>: A depth-2 beam search estimate obtained by simulating the candidate move, sampling a single subsequent piece and selecting the highest-scoring terminal placement according to the same evaluation criteria. This provides an approximate measure of immediate downstream potential.

      </ol>
      <p>
        Both metrics are min–max normalized to the [0,1] interval and combined via a linear mixture: <code>score = α·flow + (1−α)·beam</code>, with α empirically set to 0.7. This hybridization preserves diversity from the GFlowNet while injecting a targeted lookahead bias, effectively reducing the search space.
      </p>
      <p>
        Finally, moves are categorized by line clears, net hole reduction and clean-flat outcomes. A shortlist of the top three moves is selected from the highest-scoring category and presented to the user, balancing exploration through GFlowNet sampling and exploitation via heuristic prioritization.
      </p>
    </details>    
      
  
    <div class="references">
      <h3>Further Reading – Tetris GFlowNet Demo</h3>
      <ul>
        <li>
          <strong>[1]</strong>
          Pan, L. et al.
          <a href="https://arxiv.org/pdf/2302.09465.pdf" target="_blank">
            <em>Stochastic Generative Flow Networks</em>
          </a>
          (arXiv 2023)
        </li>
        <li>
          <strong>[2]</strong>
          Jain, M. et al.
          <a href="https://arxiv.org/pdf/2203.04115.pdf" target="_blank">
            <em>Biological Sequence Design with GFlowNets</em>
          </a>
          (arXiv 2022)
        </li>
      </ul>
    </div>
  </section>
  


  <section id="motivation">
    <div class="section-content">
      <h2>3. Understanding GFlowNets</h2>
      
      <p>
        GFlowNets learn to generate different solutions by giving each one a chance 
        based on how “good” it is. If a solution is better (or has a higher “reward”), 
        the GFlowNet will pick it more often. This also leaves room for other strong 
        solutions, so you do not get stuck with just a single “winner.”
      </p>
  
      <p>
        A helpful way to picture this is to imagine water flowing through a network 
        of pipes. Each node in the network is a partial step toward a final solution. 
        Each edge is a possible move that sends some of the water forward. A reward 
        acts like the size of the container at the end of each path: bigger rewards 
        gather more water, so those solutions appear more frequently. Smaller rewards 
        still collect some water too, which prevents them from being ignored.
      </p>
  
      <p>
        This design relies on <em>flow conservation</em>. It means any flow that enters 
        a node must exit it along one of the available paths. Nothing is lost or created 
        midstream. In practice, this simple rule ensures that high-reward solutions end 
        up with more flow, yet other options receive some flow as well. The diagram below 
        illustrates how different paths share in this flow of probability, making 
        GFlowNets valuable for tasks that need multiple strong outcomes.
      </p>
      
      <!-- Flow Conservation Demo -->
      <div id="flowConservationContainer">
        <svg id="flowConservationSVG"></svg>
      </div>
      <script src="/static/flow_conservation.js"></script>
      <script>
        document.addEventListener("DOMContentLoaded", function() {
          initFlowConservationDemo();
        });
      </script>
  
      <div class="references">
        <h3>Further Readings on GFlowNets</h3>
        <ul>
          <li>
            <strong>[1]</strong>
            Tiapkin, D. et al.
            <a href="https://arxiv.org/pdf/2310.12934.pdf" target="_blank">
              <em>Generative Flow Networks as Entropy-Regularized RL</em>
            </a>
            (arXiv 2023)
          </li>
          <li>
            <strong>[2]</strong>
            Zhang, D. et al.
            <a href="https://arxiv.org/pdf/2209.02606.pdf" target="_blank">
              <em>Unifying Generative Models with GFlowNets and Beyond</em>
            </a>
            (arXiv 2022)
          </li>
        </ul>
      </div>
    </div>
  </section>
  
  
  

  

  <!-- =================================================================================
SECTION 4: GFlowNet Core Concepts & Flow Conservation with Molecule Visualization
================================================================================= -->
<section id="theory">
  <div class="section-content">
    <h2>4. Core Concepts & Flow Conservation</h2>
    
    <p>
      In this section, we take a look at how GFlowNets can help build complex objects like molecules. 
      The idea is to treat each molecule as a path in a Directed Acyclic Graph (DAG), where each node represents 
      a partial structure and each edge represents an action that adds or changes something about that structure. 
      You progress step by step until you reach a fully formed molecule at a terminal node.
    </p>
    
    <p>
      The principle of <strong>flow conservation</strong> ensures that any flow entering a node must be 
      distributed among its outgoing edges. A molecule with a higher “reward” (for instance, a better prediction 
      for binding to a target protein) naturally receives more flow. This means GFlowNets are not locked onto 
      one single candidate; they encourage multiple promising solutions. That diversity can be crucial when 
      searching large spaces, such as all possible molecules you might want to synthesize and test.
    </p>
    
    <!-- BIG DAG in #moleculeFlowSVG (molecule_flow.js) -->
    <div id="moleculeFlowContainer">
      <svg id="moleculeFlowSVG" width="1000" height="600"></svg>
      <div
        id="flowTooltipBig"
        style="position:absolute; background:#333; color:#fff; padding:6px;
               border-radius:4px; pointer-events:none; opacity:0;"
      >
      </div>
    </div>

    <style>
      #moleculeFlowContainer {
        max-width: 1000px;
        margin: 0 auto;
        padding: 0;
        text-align: center;
      }

      #moleculeFlowSVG {
        display: block;
        margin: 0 auto;
      }
    </style>

    <script src="static/molecule_flow.js"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        initMoleculeFlowDemo();
      });
    </script>

    <details>
      <summary>Interpreting the Diagram</summary>
      <p>
        The flow starts from a single origin and immediately fans out into three main pathways—each representing a broad molecular backbone. The width of each path shows how much of the total 100% “probability mass” goes that way (roughly 55%, 37%, and 8%).
      </p>
      <p>
        Each of those three backbones then splits again into three more specific molecule designs. Along each branch you’ll see labeled flows (e.g. 28, 14, 13, etc.), which translate directly into sampling frequencies: “28” means that particular candidate will appear about 28% of the time across many draws.
      </p>
      <p>
 This two-stage layout makes it easy to see which general structures the model prefers, while still keeping lower-probability options in view.
      </p>
    </details>
    
    

    <details>
      <summary>Why This Matters in Drug Discovery</summary>
      <p>
        Drug discovery involves exploring many possible molecular structures 
        in hopes of finding effective treatments. Because GFlowNets distribute 
        “flow” across a set of strong candidates, they can systematically 
        uncover molecules with good properties without missing other viable options. 
        If a top molecule fails in real testing, the method still has other strong 
        candidates to propose.      </p>
      <p>
        This ability to handle uncertainty in scoring (for example, uncertain predictions 
        of how well a molecule binds) makes GFlowNets a powerful tool. Instead of discarding 
        lower-scoring molecules outright, some flow will still reach them, keeping 
        potential alternatives in play. Moreover, scientists also obtain samples with lower rewards, which can further enhance their understanding of the molecule generation process using GFlowNets. 

      </p>
    </details>

    <details>
      <summary>Technical Insight: Flow = Reward at Terminal Nodes</summary>
      <p>
        When the DAG reaches a terminal node that represents a complete molecule, 
        the amount of flow arriving there matches the reward for that molecule. 
        This ensures that the fraction of flow allocated to each final structure 
        aligns with how promising that molecule is, based on the reward function. 
        Training a GFlowNet involves adjusting these flows so that the final 
        distribution over molecules reflects their rewards.
      </p>
    </details>

    <details>
      <summary>BONUS: Pretraining GFlowNets for Fast Inference</summary>
      <p>
        One of the key benefits of GFlowNets is that they can be pretrained. 
        You spend the computation time upfront and once the network has learned, 
        it can produce diverse, high-quality candidates almost instantly. This 
        speeds up tasks like generating large batches of promising molecules 
        during the exploration phase of drug development.
      </p>
      <p>
        Compared to traditional methods that require extensive searching for each new design, 
        a pretrained GFlowNet quickly provides a broad set of potentially strong solutions 
        without repeated heavy computations.
      </p>
    </details>

    <div class="references">
      <h3>Further Reading – Core Concepts & Flow Conservation</h3>
      <ul>
        <li>
          <strong>[1]</strong>
          Malkin, N. et al.
          <a href="https://arxiv.org/pdf/2201.13259.pdf" target="_blank">
            <em>Trajectory Balance: Improved Credit Assignment in GFlowNets</em>
          </a>
          (ICML 2022)
        </li>
        <li>
          <strong>[2]</strong>
          Madan, K. et al.
          <a href="https://arxiv.org/pdf/2209.12782.pdf" target="_blank">
            <em>Learning GFlowNets from Partial Episodes for Improved Convergence and Stability</em>
          </a>
          (arXiv 2022)
        </li>
        <li>
          <strong>[3]</strong>
          Leguy, J., Cauchy, T., Glavatskikh, M., Duval, B., &amp; Da Mota, B. 
          <a href="https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00458-z" target="_blank">
            <em>EvoMol: a flexible and interpretable evolutionary algorithm for unbiased de novo molecular generation</em>
          </a>
          <em>Journal of Cheminformatics</em> 12, 58 (2020). Open Access. :contentReference[oaicite:0]{index=0}
        </li>
      </ul>
    </div>
    

  </div>
</section>

<section id="advanced-math">
  <div class="section-content">
    <h2>5. Advanced Math</h2>
    <p>
      Below are expansions with more formal definitions, training objectives like 
      Trajectory Balance, Flow Matching, Detailed Balance and references to the 
      original derivations. Expand any if you want the rigorous details.
    </p>

    <!-- Flow Formulas & Probability Calculation -->
    <details>
      <summary>Flow Formulas &amp; Probability Calculation</summary>
      <p>
        Let $F(s \to s')$ be the flow from state $s$ to state $s'$.  
        In a non-terminal state $s$:
      </p>
      <!-- Display mode equation with double $$ -->
      $$\sum_{\text{children of } s} F\bigl(s \to s_{\text{child}}\bigr) 
        \;=\; 
        \sum_{\text{parents of } s} F\bigl(s_{\text{parent}} \to s\bigr).$$

      <p>
        For a terminal state $x$, the total inflow equals its reward:  
        $$\sum_{\text{parents of } x} F(\ldots) \;=\; R(x).$$
        Sampling $x$ with probability 
        $$\frac{R(x)}{\sum R(x')}$$ 
        follows if we treat each $F(s \to s')$ as unnormalized probabilities 
        forming a policy.
      </p>
    </details>

    <!-- Trajectory Balance (TB) -->
    <details>
      <summary>Trajectory Balance (TB)</summary>
      <p>
        TB ties the forward probabilities along a trajectory to the reward $R(x)$ 
        and the backward probabilities. Specifically, if  
        $$\tau = (s_0 \to s_1 \to \dots \to s_n = x)$$
        is a path to terminal $x$, TB wants:
      </p>
      $$\frac{P_F(\tau)}{Q(\tau)} \;=\; \frac{R(x)}{Z},$$

      <p>
        where $Q(\tau)$ is the backward or reverse path probability and $Z$ 
        is the partition function (the sum of all $R(x)$).  
        This elegantly ensures each trajectory’s total probability mass 
        aligns with the reward.
      </p>
    </details>

    <!-- Flow Matching & Detailed Balance -->
    <details>
      <summary>Flow Matching &amp; Detailed Balance</summary>
      <p>
        Flow Matching is a local constraint approach: for each state $s$, 
        <em>incoming flow = outgoing flow</em>. Detailed Balance is a pairwise 
        condition that ensures symmetrical consistency across edges, also leading 
        to a consistent overall distribution.
      </p>
      <p>
        Both yield solutions that sample final states in proportion to $R(x)$, but 
        TB can handle longer trajectories more effectively, often reducing variance.
      </p>
    </details>

    <!-- References -->
    <div class="references">
      <h3>Further Reading – Advanced Math</h3>
      <ul>
        <li>
          <strong>[1]</strong>
          Zimmermann, H. et al.
          <a href="https://arxiv.org/pdf/2210.07992.pdf" target="_blank">
            <em>A Variational Perspective on Generative Flow Networks</em>
          </a>
          (arXiv 2022)
        </li>
        <li>
          <strong>[2]</strong>
          Jiralerspong, M. et al.
          <a href="https://arxiv.org/pdf/2310.02779.pdf" target="_blank">
            <em>Expected Flow Networks in Stochastic Environments and Two-Player Zero-Sum Games</em>
          </a>
          (arXiv 2023)
        </li>
        <li>
          <strong>[3]</strong>
          Li, C. et al.
          <a href="https://arxiv.org/pdf/2406.01901.pdf" target="_blank">
            <em>Bifurcated Generative Flow Networks</em>
          </a>
          (arXiv 2024)
        </li>
      </ul>
    </div>
    
</section>

  
</main>

<!-- (Line 830) FOOTER -->
<footer>
  <p>&copy; 2025 – Understanding Generative Flow Networks (GFlowNets) - A User perspective</p>
</footer>

<script src="https://cdn.jsdelivr.net/npm/particles.js@2.0.0/particles.min.js"></script>
<script src="https://d3js.org/d3.v7.min.js"></script>
<script src="https://unpkg.com/d3-sankey@0.12.3/dist/d3-sankey.min.js"></script>


<script>

  "use strict";
  
  // ====================== CONSTANTS & GLOBALS ======================
  const CELL_SIZE = 30;
  const COLS = 10;
  const ROWS = 20;
  const TICK_INTERVAL = 400;
  const MOVE_PAUSE_DURATION = 1000;
  
  // Game/Agent
  let game = null;
  let agent = null;    // old TrajectoryBalanceAgent
  let nnModel = null;  // will hold NeuralFlowNet if you ever train one
  let trajectory = [];
  
  // UI handles
  let canvas, ctx, candidateListEl, resetBtn;
  
  // State
  let currentGameState = null;
  let currentPieceCenter = { x:0, y:0 };
  let candidateMoves = [];
  let topCandidates = [];
  let appliedArrows = [];
  let particles = [];
  let simulationPaused = false;
  let lastPieceId = null;
  let particleSpawnAccumulator = 0;
  let lastTime = performance.now();
    
    // ====================== TETROMINO DEFINITIONS ======================
    const TETROMINOES = {
      I: [
        [0,0,0,0],
        [1,1,1,1],
        [0,0,0,0],
        [0,0,0,0]
      ],
      O: [
        [1,1],
        [1,1]
      ],
      T: [
        [0,1,0],
        [1,1,1],
        [0,0,0]
      ],
      S: [
        [0,1,1],
        [1,1,0],
        [0,0,0]
      ],
      Z: [
        [1,1,0],
        [0,1,1],
        [0,0,0]
      ],
      J: [
        [1,0,0],
        [1,1,1],
        [0,0,0]
      ],
      L: [
        [0,0,1],
        [1,1,1],
        [0,0,0]
      ]
    };
    const PIECES = Object.keys(TETROMINOES);
    const PIECE_IDS = {};
    PIECES.forEach((p,i)=> { PIECE_IDS[p] = i; });
    
    // ====================== HELPER FUNCTIONS ======================
    function deepCopy(matrix) {
      return JSON.parse(JSON.stringify(matrix));
    }
    function rotateMatrix(matrix) {
      const rows = matrix.length;
      const cols = matrix[0].length;
      const rotated = [];
      for (let c=0; c<cols; c++){
        rotated[c] = [];
        for (let r=rows-1; r>=0; r--){
          rotated[c].push(matrix[r][c]);
        }
      }
      return rotated;
    }
    function hexToRgb(hex) {
      hex = hex.replace(/^#/, "");
      const bigint = parseInt(hex, 16);
      return {
        r: (bigint >> 16) & 255,
        g: (bigint >> 8) & 255,
        b: bigint & 255
      };
    }
   
      function beamSearch(moves, depth, beamWidth) {
        return moves.reduce((best, m) =>
          m.combinedScore > best.combinedScore ? m : best
        , moves[0]);
      }

  /**
 * Simulate dropping `piece` on arbitrary `board`, clear lines,
 * and return the resulting board + linesCleared.
 */
function simulatePlacementOnBoard(board, piece) {
  const b = deepCopy(board);
  const shape = piece.shape;
  for (let r = 0; r < shape.length; r++) {
    for (let c = 0; c < shape[r].length; c++) {
      if (shape[r][c]) {
        b[piece.y + r][piece.x + c] = 1;
      }
    }
  }
  const newBoard = b.filter(row => !row.every(cell => cell === 1));
  const linesCleared = b.length - newBoard.length;
  for (let i = 0; i < linesCleared; i++) {
    newBoard.unshift(new Array(COLS).fill(0));
  }
  return { newBoard, linesCleared };
}

/**
 * Depth‑2 “beamEvaluate”: for a given first‐move `m`, score =
 * heuristic(m on current board)
 *   + max₂ [ heuristic(m₂ on board after m) ]
 * We don’t know the next piece, so we sample *one* via spawn_piece.
 */
 function beamEvaluate(m) {
  // 1) Apply first move
  const { newBoard: b1, linesCleared: lc1 } =
    simulatePlacementOnBoard(game.board, m.piece);
  const score1 =
      TETRIS_WEIGHTS.agg   * aggregateHeight(b1)
    + TETRIS_WEIGHTS.lines * lc1
    + TETRIS_WEIGHTS.holes * countHoles(b1)
    + TETRIS_WEIGHTS.bump  * countBumpiness(b1);

  // 2) Sample one “next piece” on b1 via a real TetrisGame
  const tNext = new TetrisGame();
  tNext.board = deepCopy(b1);
  tNext.rows  = ROWS;
  tNext.cols  = COLS;
  const nextPiece = tNext.spawn_piece();     // this.collides() now works
  tNext.current_piece = nextPiece;

  // 3) Evaluate *all* terminal moves of that next piece, pick the best
  let best2 = -Infinity;
  tNext.get_terminal_moves().forEach(m2 => {
    const { newBoard: b2, linesCleared: lc2 } =
      simulatePlacementOnBoard(b1, m2.piece);
    const sc2 =
        TETRIS_WEIGHTS.agg   * aggregateHeight(b2)
      + TETRIS_WEIGHTS.lines * lc2
      + TETRIS_WEIGHTS.holes * countHoles(b2)
      + TETRIS_WEIGHTS.bump  * countBumpiness(b2);
    if (sc2 > best2) best2 = sc2;
  });

  return score1 + best2;
}


  




    // ====================== TETRIS GAME CLASS ======================
// Helper: gravity drop for any floating blocks
function applyGravity(board) {
  for (let c = 0; c < COLS; c++) {
    const stack = [];
    for (let r = ROWS - 1; r >= 0; r--) {
      if (board[r][c] === 1) stack.push(1);
    }
    for (let r = ROWS - 1; r >= 0; r--) {
      board[r][c] = stack.length ? stack.shift() : 0;
    }
  }
}

// ====================== TETRIS GAME CLASS ======================
// ====================== TETRIS GAME CLASS ======================
class TetrisGame {
  constructor(cols=COLS, rows=ROWS){
    this.cols = cols;
    this.rows = rows;
    this.reset_game();
  }
  reset_game(){
    this.board = [];
    for(let r=0; r<this.rows; r++){
      this.board.push(new Array(this.cols).fill(0));
    }
    this.score = 0;
    this.game_over = false;
    this.piece_id = 0;
    this.current_piece = this.spawn_piece();
    this.target_piece = null;
    this.cached_moves = null;
    this._cached_state_key = null;
  }
  spawn_piece(){
    this.piece_id += 1;
    const tTypes = Object.keys(TETROMINOES);
    const t_type = tTypes[Math.floor(Math.random()*tTypes.length)];
    const shape = deepCopy(TETROMINOES[t_type]);
    const piece = {
      type: t_type,
      shape: shape,
      x: Math.floor((this.cols - shape[0].length)/2),
      y: 0
    };
    // check collision
    if(this.collides(piece)){
      this.game_over = true;
    }
    return piece;
  }
  collides(piece){
    const shape = piece.shape;
    for(let r=0; r<shape.length; r++){
      for(let c=0; c<shape[r].length; c++){
        if(shape[r][c]){
          const x = piece.x+c;
          const y = piece.y+r;
          if(x<0 || x>=this.cols || y>=this.rows){
            return true;
          }
          if(y>=0 && this.board[y][x]){
            return true;
          }
        }
      }
    }
    return false;
  }
  clear_lines(){
    const new_board = [];
    for(let r=0; r<this.board.length; r++){
      if(!this.board[r].every(cell => cell===1)){
        new_board.push(this.board[r]);
      }
    }
    const cleared = this.rows - new_board.length;
    for(let i=0; i<cleared; i++){
      new_board.unshift(new Array(this.cols).fill(0));
    }
    this.board = new_board;
    this.score += cleared;
    return cleared;
  }
  lock_piece(){
    const p = this.current_piece;
    for(let r=0; r<p.shape.length; r++){
      for(let c=0; c<p.shape[r].length; c++){
        if(p.shape[r][c]){
          const x = p.x + c;
          const y = p.y + r;
          if(x>=0 && x<this.cols && y>=0 && y<this.rows){
            this.board[y][x] = 1;
          }
        }
      }
    }
    this.clear_lines();
    this.current_piece = this.spawn_piece();
    this.target_piece = null;
    this.cached_moves = null;
    this._cached_state_key = null;
  }
  lock_target(){
    if(!this.target_piece) return;
    const p = this.target_piece;
    for(let r=0; r<p.shape.length; r++){
      for(let c=0; c<p.shape[r].length; c++){
        if(p.shape[r][c]){
          const x = p.x + c;
          const y = p.y + r;
          if(x>=0 && x<this.cols && y>=0 && y<this.rows){
            this.board[y][x] = 1;
          }
        }
      }
    }
    this.clear_lines();
    this.current_piece = this.spawn_piece();
    this.target_piece = null;
    this.cached_moves = null;
    this._cached_state_key = null;
  }
  get_piece_center(piece=null){
    if(!piece) piece = this.current_piece;
    if(!piece || !piece.shape) return { x:0, y:0 };
    const h = piece.shape.length;
    const w = piece.shape[0].length;
    return {
      x: (piece.x + w/2)*CELL_SIZE,
      y: (piece.y + h/2)*CELL_SIZE
    };
  }
  get_state_key(){
    const piece = this.current_piece;
    const stateObj = {
      board: this.board,
      piece: {
        type: piece.type,
        shape: piece.shape,
        x: piece.x,
        y: piece.y
      }
    };
    return JSON.stringify(stateObj);
  }
  get_terminal_moves(){
    if(this.game_over){
      return [];
    }
    const current_state_key = this.get_state_key();
    if(this.cached_moves && this._cached_state_key===current_state_key){
      return this.cached_moves;
    }
    const orig = this.current_piece;
    const base_shape = TETROMINOES[orig.type];
    const candidates = [];
    const rotations = (orig.type==="O") ? [0] : [0,1,2,3];
    for(let rot of rotations){
      let shape = deepCopy(base_shape);
      for(let i=0; i<rot; i++){
        shape = rotateMatrix(shape);
      }
      const w = shape[0].length;
      for(let x=0; x<=this.cols-w; x++){
        const testPiece = {
          type: orig.type,
          shape: deepCopy(shape),
          x: x,
          y: 0
        };
        if(this.collides(testPiece)){
          continue;
        }
        let y=0;
        while(!this.collides({...testPiece, y:y}) && y<this.rows){
          y++;
        }
        testPiece.y = y-1;
        if(testPiece.y<0) continue;
        const center = this.get_piece_center(testPiece);
        const action_key = `r${rot}_x${x}`;
        candidates.push({
          action_key: action_key,
          piece: testPiece,
          piece_center: center
        });
      }
    }
    this.cached_moves = candidates;
    this._cached_state_key = current_state_key;
    return candidates;
  }
  tick(){
    if(this.game_over) return;
    if(this.target_piece){
      this.current_piece.shape = deepCopy(this.target_piece.shape);
      this.cached_moves = null;
      this._cached_state_key = null;
      const px = this.current_piece.x;
      const py = this.current_piece.y;
      const tx = this.target_piece.x;
      const ty = this.target_piece.y;
      if(px<tx) this.current_piece.x++;
      else if(px>tx) this.current_piece.x--;
      if(py<ty) this.current_piece.y++;
      else if(py>ty) this.current_piece.y--;
      if(this.current_piece.x===tx && this.current_piece.y===ty){
        this.lock_target();
      }
    } else {
      const nextP = {
        type: this.current_piece.type,
        shape: this.current_piece.shape,
        x: this.current_piece.x,
        y: this.current_piece.y+1
      };
      if(!this.collides(nextP)){
        this.current_piece.y++;
      } else {
        this.lock_piece();
      }
    }
  }
  is_over(){
    return this.game_over;
  }
  get_final_reward(){
    if(this.game_over){
      return this.score*10 - 10;
    } else {
      return this.score*10;
    }
  }
}
// gravity helper (if needed elsewhere)
function applyGravity(board) {
  for (let c=0;c<COLS;c++) {
    const stack = [];
    for (let r=ROWS-1;r>=0;r--) if (board[r][c]) stack.push(1);
    for (let r=ROWS-1;r>=0;r--) board[r][c] = stack.shift()||0;
  }
}


    // ====================== TRAJECTORY BALANCE AGENT (old) ======================
    class TrajectoryBalanceAgent {
      constructor(lr=0.01){
        this.log_flows = {};
        this.logZ = 0.0;
        this.lr = lr;
      }
      _ensure_action_exists(state_key, action_key){
        if(!this.log_flows[state_key]){
          this.log_flows[state_key] = {};
        }
        if(!this.log_flows[state_key][action_key]){
          const val = 0.5 + Math.random();
          this.log_flows[state_key][action_key] = Math.log(val);
        }
      }
      sample_action(state_key, candidates){
        for(let c of candidates){
          this._ensure_action_exists(state_key, c.action_key);
        }
        const logValues = candidates.map(c => this.log_flows[state_key][c.action_key]);
        const max_log = Math.max(...logValues);
        const exps = logValues.map(lv => Math.exp(lv - max_log));
        const sum_exps = exps.reduce((a,b)=>a+b,0);
        const probs = exps.map(e=> e/sum_exps);
        const r = Math.random();
        let cum=0, idx=0;
        for(let i=0; i<probs.length; i++){
          cum += probs[i];
          if(r<=cum){
            idx=i; break;
          }
        }
        return [candidates[idx], probs[idx]];
      }
      get_log_p_action(state_key, action_key){
        this._ensure_action_exists(state_key, action_key);
        const all_logs = Object.values(this.log_flows[state_key]);
        const max_val = Math.max(...all_logs);
        const sum_exp = all_logs.reduce((acc,x)=> acc+Math.exp(x - max_val), 0);
        const denom = Math.log(sum_exp)+max_val;
        const numerator = this.log_flows[state_key][action_key];
        return numerator - denom;
      }
      update_trajectory(traj, final_reward){
        if(final_reward<=0) final_reward=0.01;
        const logR = Math.log(final_reward);
        let sum_logp=0;
        for(let [s,a] of traj){
          sum_logp += this.get_log_p_action(s,a);
        }
        const target = logR - this.logZ;
        const diff = sum_logp - target;
        // update logZ
        this.logZ += this.lr*diff;
        // update flows
        for(let [s,a] of traj){
          this.log_flows[s][a] -= this.lr*diff;
        }
      }
      loadFromJSON(obj){
        try {
          this.log_flows = obj.log_flows || obj.weights || {};
          this.logZ = obj.logZ || 0.0;
          console.log("[agent] Loaded from JSON. (#states=", 
              Object.keys(this.log_flows).length, ", logZ=", this.logZ, ")");
        } catch(e){
          console.error("[agent] Error loading from JSON:", e);
        }
      }
    }
    
    // ====================== NEURAL NET REPLACEMENT ======================
    // (intentionally left in place but will never load)—unchanged
    class NeuralFlowNet {
      constructor() {
        this.logZ = 0.0;
        this.loaded = false;
        this.sharedW = null;
        this.sharedb = null;
        this.flowW   = null;
        this.flowb   = null;
        this.advW    = null;
        this.advb    = null;
      }
      loadJson(o) {
        this.logZ = o.logZ;
        const w = o.weights;
        this.sharedW = w["shared.0.weight"];
        this.sharedb = w["shared.0.bias"];
        this.flowW   = w["state_flow.weight"];
        this.flowb   = w["state_flow.bias"];
        this.advW    = w["advantage.weight"];
        this.advb    = w["advantage.bias"];
        this.loaded = true;
        console.log("[NeuralFlowNet] loaded logZ=", this.logZ);
      }
      _linear(W, b, x) {
        const out = new Float32Array(b.length);
        const xLen = x.length;
        for (let i = 0; i < b.length; i++) {
          let sum = b[i];
          const row = W[i];
          for (let j = 0; j < row.length; j++) {
            const xv = j < xLen ? x[j] : 0;
            sum += row[j] * xv;
          }
          out[i] = sum;
        }
        return out;
      }
      forward(sa) {
        if (!this.loaded) {
          throw new Error("NeuralFlowNet.forward() called before loadJson()");
        }
        let h = this._linear(this.sharedW, this.sharedb, sa);
        for (let i = 0; i < h.length; i++) if (h[i] < 0) h[i] = 0;
        const z = this._linear(this.flowW, this.flowb, h)[0];
        const a2 = sa.slice(sa.length - 2);
        const ha = new Float32Array(h.length + 2);
        ha.set(h, 0);
        ha.set(a2, h.length);
        const adv = this._linear(this.advW, this.advb, ha)[0];
        return z + adv;
      }
    }
    
    // ====================== ENCODING HELPERS ======================
    function flattenBoard(board){
      let arr=[];
      for (let r=0; r<board.length; r++){
        arr.push(...board[r]);
      }
      return arr;
    }
    function countHoles(board){
      let holes=0;
      let rows= board.length;
      let cols= board[0].length;
      for (let c=0; c<cols; c++){
        let blockFound=false;
        for (let r=0; r<rows; r++){
          if (board[r][c]===1) blockFound=true;
          else if (blockFound && board[r][c]===0) holes++;
        }
      }
      return holes;
    }
    function boardMaxHeight(board){
      let rows= board.length;
      for (let r=0; r<rows; r++){
        if (board[r].some(cell=>cell===1)) return rows-r;
      }
      return 0;
    }
    function countBumpiness(board){
      let rows= board.length;
      let cols= board[0].length;
      let heights=[];
      for (let c=0;c<cols;c++){
        let h=0;
        for (let r=0;r<rows;r++){
          if (board[r][c]===1){ h= rows-r; break; }
        }
        heights.push(h);
      }
      let bump=0;
      for (let c=0;c<cols-1;c++) bump+= Math.abs(heights[c]- heights[c+1]);
      return bump;
    }
    function encodeState(board, piece){
      let flat= flattenBoard(board);
      let oh= new Array(7).fill(0);
      let idx= PIECE_IDS[piece.type] || 0;
      oh[idx]=1;
      let px= piece.x/(COLS-1);
      let py= piece.y/(ROWS-1);
      let holes= countHoles(board);
      let maxH= boardMaxHeight(board);
      let bump= countBumpiness(board);
      return flat.concat( oh, [px,py], [holes,maxH,bump] );
    }
    function encodeAction(aKey){
      let [rPart,xPart]= aKey.split("_");
      let rVal= parseInt(rPart.slice(1))/3.0;
      let xVal= parseInt(xPart.slice(1))/(COLS-1);
      return [rVal,xVal];
    }
    function encodeStateAction(board, piece, actionKey) {
      let features = encodeState(board, piece).concat(encodeAction(actionKey));
      const required = (nnModel && nnModel.loaded && nnModel.sharedW)
        ? nnModel.sharedW[0].length
        : features.length;
      if (features.length < required) {
        features = features.concat(
          new Array(required - features.length).fill(0)
        );
      }
      return features;
    }
    
    // ====================== VISUAL EFFECTS ======================
    class Particle { constructor(x,y,vx,vy,radius=4,life=1.0,color={r:255,g:255,b:255}){ this.x=x; this.y=y; this.vx=vx; this.vy=vy; this.radius=radius; this.life=life; this.color=color;} update(dt){this.x += this.vx * dt; this.y += this.vy * dt; this.life -= dt * 0.4;} draw(ctx){ ctx.beginPath(); ctx.arc(this.x,this.y,this.radius,0,2*Math.PI); let grad=ctx.createRadialGradient(this.x,this.y,this.radius/2,this.x,this.y,this.radius); grad.addColorStop(0,`rgba(${this.color.r},${this.color.g},${this.color.b},${this.life})`); grad.addColorStop(1,`rgba(${this.color.r},${this.color.g},${this.color.b},0)`); ctx.fillStyle=grad; ctx.fill(); }}
    class Arrow { constructor(from,to,flow,color="#66ff66"){this.from=from;this.to=to;this.flow=flow;this.color=color;this.life=1.0;} update(dt){this.life -= dt*0.5;} draw(ctx){const rgb=hexToRgb(this.color);ctx.strokeStyle=`rgba(${rgb.r},${rgb.g},${rgb.b},${0.8*this.life})`;ctx.beginPath();ctx.moveTo(this.from.x,this.from.y);ctx.lineTo(this.to.x,this.to.y);ctx.stroke();let angle=Math.atan2(this.to.y - this.from.y,this.to.x - this.from.x);ctx.beginPath();ctx.moveTo(this.to.x,this.to.y);ctx.lineTo(this.to.x - 10*Math.cos(angle - Math.PI/6),this.to.y - 10*Math.sin(angle - Math.PI/6));ctx.lineTo(this.to.x - 10*Math.cos(angle + Math.PI/6),this.to.y - 10*Math.sin(angle + Math.PI/6));ctx.closePath();ctx.fillStyle=`rgba(${rgb.r},${rgb.g},${rgb.b},${0.8*this.life})`;ctx.fill();}}
    
    // ====================== "API" LOGIC (FRONT-END) ======================
    function getCandidateMoves() {
      const cands = game.get_terminal_moves();
      if (nnModel && nnModel.loaded) {
        let sumFlow = 0;
        for (let c of cands) {
          const stateVec = encoder.encode(game.board, game.current_piece.type);
          const [rVal, xVal] = encodeAction(c.action_key);
          const actFeat = new Float32Array([rVal, xVal]);
          const sa = new Float32Array(stateVec.length + 2);
          sa.set(stateVec, 0);
          sa.set(actFeat, stateVec.length);
          let logF = nnModel.forward(sa) / τ;
          logF = Math.max(-20, Math.min(20, logF));
          const f = Math.exp(logF);
          c.flow = f;
          sumFlow += f;
        }
        for (let c of cands) {
          c.probability = sumFlow > 0 ? c.flow / sumFlow : 1 / cands.length;
        }
      } else {
        const state_key = game.get_state_key();
        let sumExp = 0;
        for (let c of cands) {
          agent._ensure_action_exists(state_key, c.action_key);
          const fv = Math.exp(agent.log_flows[state_key][c.action_key]);
          sumExp += fv;
        }
        for (let c of cands) {
          const fv = Math.exp(agent.log_flows[state_key][c.action_key]);
          c.flow = fv;
          c.probability = sumExp > 0 ? fv / sumExp : 1.0 / cands.length;
        }
      }
      return {
        current_piece_center: game.get_piece_center(),
        terminal_moves: cands,
        game_state: { board: game.board, current_piece: game.current_piece, score: game.score, game_over: game.game_over }
      };
    }
    
    function selectMove(actionKey=null){
      if(game.is_over()){ return { error:"Game Over" }; }
      const cands = game.get_terminal_moves();
      if(!cands || cands.length===0){ return { error:"No moves" }; }
      let selected_action=null;
      if(actionKey){ selected_action = cands.find(x=>x.action_key===actionKey) || null; }
      if(!selected_action){
        if(nnModel && nnModel.loaded){
          cands.sort((a,b)=> (b.flow||0) - (a.flow||0));
          selected_action= cands[0];
        } else {
          const state_key = game.get_state_key();
          const [cand,_p] = agent.sample_action(state_key, cands);
          selected_action=cand;
        }
      }
      trajectory.push([game.get_state_key(), selected_action.action_key]);
      game.target_piece = deepCopy(selected_action.piece);
      const arrow_info={ from: game.get_piece_center(game.current_piece), to: selected_action.piece_center, flow: selected_action.flow||0, probability: selected_action.probability||0 };
      return { action_key: selected_action.action_key, arrow: arrow_info, game_state: { board: game.board, current_piece: game.current_piece, score: game.score, game_over: game.game_over, piece_id: game.piece_id }};
    }
    function tickGameLogic() {
      const oldGameOver = game.is_over();
      const oldPieceId  = game.piece_id;
      game.tick();
      const newGameOver = game.is_over();
      const newPieceId  = game.piece_id;
      if (newGameOver && !oldGameOver) {
        const finalReward = game.get_final_reward();
        agent.update_trajectory(trajectory, finalReward);
        trajectory = [];
        game.reset_game();
      }
      let terminal_moves = [];
      if (newPieceId !== oldPieceId && !game.is_over()) {
        const cands = game.get_terminal_moves();
        if (nnModel && nnModel.loaded) {
          let sumFlow = 0;
          for (let c of cands) {
            const stateVec = encoder.encode(game.board, game.current_piece.type);
            const [rVal, xVal] = encodeAction(c.action_key);
            const sa = new Float32Array(stateVec.length + 2);
            sa.set(stateVec, 0);
            sa.set([rVal, xVal], stateVec.length);
            let logF = nnModel.forward(sa);
            logF = Math.max(-20, Math.min(20, logF));
            const f = Math.exp(logF);
            c.flow = f;
            sumFlow += f;
          }
          for (let c of cands) {
            c.probability = sumFlow > 0 ? c.flow / sumFlow : 1.0 / cands.length;
            terminal_moves.push(c);
          }
        } else {
          const stateKey = game.get_state_key();
          let sumExp = 0;
          for (let c of cands) {
            agent._ensure_action_exists(stateKey, c.action_key);
            const val = Math.exp(agent.log_flows[stateKey][c.action_key]);
            sumExp += val;
          }
          for (let c of cands) {
            const val = Math.exp(agent.log_flows[stateKey][c.action_key]);
            c.flow = val;
            c.probability = sumExp > 0 ? val / sumExp : 1.0 / cands.length;
            terminal_moves.push(c);
          }
        }
      }
      return { game_state: { board: game.board, current_piece: game.current_piece, score: game.score, game_over: game.game_over, piece_id: game.piece_id }, current_piece_center: game.get_piece_center(), terminal_moves };
    }
    function resetGameLogic() {
      game.reset_game();
      trajectory = [];
      return { status: "reset" };
  }
  
  
  // ====================== UI & ANIMATION ======================
  
  function assignCandidateColors(cands) {
      cands.forEach((cand, i) => {
          cand.color = i === 0
              ? "#33ff66"
              : i === 1
                  ? "#ffd700"
                  : i === 2
                      ? "#ff6666"
                      : "#dddddd";
      });
  }
  
  function drawBoard(gs) {
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      ctx.fillStyle = "#222";
      ctx.fillRect(0, 0, canvas.width, canvas.height);
  
      if (!gs || !gs.board) return;
  
      for (let r = 0; r < ROWS; r++) {
          for (let c = 0; c < COLS; c++) {
              if (gs.board[r][c]) {
                  ctx.fillStyle = "#666";
                  ctx.fillRect(c * CELL_SIZE, r * CELL_SIZE, CELL_SIZE, CELL_SIZE);
                  ctx.strokeStyle = "#444";
                  ctx.strokeRect(c * CELL_SIZE, r * CELL_SIZE, CELL_SIZE, CELL_SIZE);
              } else {
                  ctx.strokeStyle = "rgba(255,255,255,0.05)";
                  ctx.strokeRect(c * CELL_SIZE, r * CELL_SIZE, CELL_SIZE, CELL_SIZE);
              }
          }
      }
  }
  
  function drawCurrentPiece(gs) {
      if (!gs || !gs.current_piece) return;
  
      const piece = gs.current_piece;
      let grad = ctx.createLinearGradient(0, 0, 0, piece.shape.length * CELL_SIZE);
      grad.addColorStop(0, "#c0c0c0");
      grad.addColorStop(1, "#a0a0a0");
      ctx.fillStyle = grad;
  
      for (let r = 0; r < piece.shape.length; r++) {
          for (let c = 0; c < piece.shape[r].length; c++) {
              if (piece.shape[r][c]) {
                  const x = (piece.x + c) * CELL_SIZE;
                  const y = (piece.y + r) * CELL_SIZE;
                  ctx.fillRect(x, y, CELL_SIZE, CELL_SIZE);
                  ctx.strokeStyle = "#888";
                  ctx.strokeRect(x, y, CELL_SIZE, CELL_SIZE);
              }
          }
      }
  }
  
  function drawCandidateShadow(piece, color) {
      if (!piece || !piece.shape) return;
  
      ctx.save();
      const rgb = hexToRgb(color || "#fff");
      ctx.fillStyle = `rgba(${rgb.r},${rgb.g},${rgb.b},0.2)`;
  
      for (let r = 0; r < piece.shape.length; r++) {
          for (let c = 0; c < piece.shape[r].length; c++) {
              if (piece.shape[r][c]) {
                  const x = (piece.x + c) * CELL_SIZE;
                  const y = (piece.y + r) * CELL_SIZE;
                  ctx.fillRect(x, y, CELL_SIZE, CELL_SIZE);
              }
          }
      }
  
      ctx.restore();
  }
  
  function draw() {
      drawBoard(currentGameState);
      drawCurrentPiece(currentGameState);
  
      topCandidates.forEach(c => {
          let arr = new Arrow(currentPieceCenter, c.piece_center, c.flow, c.color);
          arr.draw(ctx);
          drawCandidateShadow(c.piece, c.color);
      });
  
      appliedArrows.forEach(a => a.draw(ctx));
      particles.forEach(p => p.draw(ctx));
  }
  
  function animate() {
      let now = performance.now();
      let dt = (now - lastTime) / 1000;
      lastTime = now;
  
      particleSpawnAccumulator += dt;
      if (particleSpawnAccumulator > 1.0) {
          if (topCandidates.length > 0) {
              let maxFlow = topCandidates[0].flow || 1;
              topCandidates.forEach((cand, i) => {
                  let ratio = cand.flow / maxFlow;
                  if (i === 0) ratio *= 2;
                  else if (i === 1) ratio *= 1;
                  else if (i === 2) ratio *= 0.3;
                  // spawnParticles(currentPieceCenter, cand.piece_center, ratio, cand.color);
              });
          }
          particleSpawnAccumulator = 0;
      }
  
      for (let i = particles.length - 1; i >= 0; i--) {
          particles[i].update(dt);
          if (particles[i].life <= 0) particles.splice(i, 1);
      }
  
      appliedArrows.forEach(a => a.update(dt));
      draw();
      requestAnimationFrame(animate);
  }
  
  function ensureMoveUniqueness(moves) {
      const seen = new Set();
  
      return moves.filter(move => {
          const coords = [];
          move.piece.shape.forEach((row, r) => {
              row.forEach((v, c) => {
                  if (v) coords.push(`${move.piece.x + c},${move.piece.y + r}`);
              });
          });
          coords.sort();
  
          const key = coords.join(";");
          if (seen.has(key)) return false;
          seen.add(key);
          return true;
      });
  }
  
  
  // ====================== FRONT-END "ENDPOINTS" ======================
  

/**
 * Classic Tetris evaluation weights (aggregateHeight, linesCleared, holes, bumpiness)
 * from the well‑known “Bertsimas & Tsitsiklis”/“Tetris Guideline” AI.
 */
 const TETRIS_WEIGHTS = {
  agg:   -4.500158825082766,
  lines:  3.4181268101392694,
  holes: -3.2178882868487753,
  bump:  -9.348695305445199
};

function aggregateHeight(board) {
  let total = 0;
  for (let c = 0; c < COLS; c++) {
    for (let r = 0; r < ROWS; r++) {
      if (board[r][c]) {
        total += ROWS - r;
        break;
      }
    }
  }
  return total;
}

/**
 * Simulate dropping `piece` onto the current `game.board`,
 * clear lines and return the resulting board + number of lines.
 */
 function simulatePlacementOnBoard(board, piece) {
  const b = deepCopy(board);
  const shape = piece.shape;
  for (let r = 0; r < shape.length; r++) {
    for (let c = 0; c < shape[r].length; c++) {
      if (shape[r][c]) {
        b[piece.y + r][piece.x + c] = 1;
      }
    }
  }
  const newBoard = b.filter(row => !row.every(cell => cell === 1));
  const linesCleared = b.length - newBoard.length;
  for (let i = 0; i < linesCleared; i++) {
    newBoard.unshift(new Array(COLS).fill(0));
  }
  return { newBoard, linesCleared };
}



function doSelectCandidate(actionKey) {
  simulationPaused = true;

  const data = selectMove(actionKey);
  if (data.error) {
    console.error("[doSelectCandidate] error:", data.error);
    simulationPaused = false;
    return;
  }

  // 1) Immediately rotate the current piece to the target shape
  game.current_piece.shape = deepCopy(data.arrow ? 
    // find the candidate that matched the arrow
    topCandidates.find(c =>
      Math.abs(c.piece_center.x - data.arrow.to.x) < 1 &&
      Math.abs(c.piece_center.y - data.arrow.to.y) < 1
    ).piece.shape
    : game.current_piece.shape
  );

  // 2) Then set up the smooth drop animation
  if (data.arrow) {
    let arrowData = data.arrow;
    let cand = topCandidates.find(c =>
      Math.abs(c.piece_center.x - arrowData.to.x) < 1 &&
      Math.abs(c.piece_center.y - arrowData.to.y) < 1
    );
    let color = cand ? cand.color : "#33ff66";
    appliedArrows.push(new Arrow(arrowData.from, arrowData.to, arrowData.flow, color));
  }

  currentGameState = data.game_state || {};
  setTimeout(() => { simulationPaused = false; }, MOVE_PAUSE_DURATION);
}

  function tickGame() {
      if (!simulationPaused) {
          const data = tickGameLogic();
          currentGameState = data.game_state || {};
          currentPieceCenter = data.current_piece_center || { x: 0, y: 0 };
          const newPieceId = currentGameState.piece_id;
  
          if (typeof newPieceId !== "undefined" && newPieceId !== lastPieceId) {
              lastPieceId = newPieceId;
              setTimeout(fetchCandidateMoves, 200);
          }
      }
  }
  
  function doResetGame() {
      resetGameLogic();
      currentGameState = null;
      currentPieceCenter = { x: 0, y: 0 };
      candidateMoves = [];
      topCandidates = [];
      appliedArrows = [];
      particles = [];
      simulationPaused = false;
      lastPieceId = null;
      candidateListEl.innerHTML = "";
      fetchCandidateMoves();
  }

// ====================== FETCH + RENDER ======================

 function clearRowsAndDrop(board, rowsToClear) {
  if (rowsToClear.length === 0) return;
  const cols = board[0].length;
  const rows = board.length;

  // 1) clear the rows
  rowsToClear.sort((a, b) => a - b);
  for (let r of rowsToClear) {
    for (let c = 0; c < cols; c++) {
      board[r][c] = 0;
    }
  }

  // 2) gravity for cells above the *highest* cleared row:
  const limit = Math.min(...rowsToClear);
  for (let c = 0; c < cols; c++) {
    // scan from just above the highest cleared row up to the top
    for (let r = limit - 1; r >= 0; r--) {
      if (board[r][c] === 1) {
        let dest = r;
        // fall until the next cell is occupied or bottom
        while (dest + 1 < rows && board[dest + 1][c] === 0) {
          dest++;
        }
        if (dest !== r) {
          board[dest][c] = 1;
          board[r][c]    = 0;
        }
      }
    }
  }
}
// helper: depth of the well in column c (count of empty cells from bottom until first block)
function computeWellDepth(board, c) {
  let depth = 0;
  for (let r = board.length - 1; r >= 0; r--) {
    if (board[r][c] === 0) depth++;
    else break;
  }
  return depth;
}

// helper: find how far down a piece at (x, y) of height h would land
function landingRow(board, x, height) {
  // start from top of board and scan down to find first filled cell
  for (let y = 0; y <= board.length - height; y++) {
    let collision = false;
    for (let dy = 0; dy < height; dy++) {
      if (board[y+dy][x]) { collision = true; break; }
    }
    if (collision) {
      // can't occupy at y because there's a block; so it must land one row above
      return y - 1;
    }
  }
  // no collision at all: lands at bottom
  return board.length - height;
}

function fetchCandidateMoves() {
  if (simulationPaused) return;

  // 1) raw moves & dedupe
  const data = getCandidateMoves();
  currentGameState   = data.game_state || {};
  currentPieceCenter = data.current_piece_center || { x:0, y:0 };
  let moves = ensureMoveUniqueness(data.terminal_moves || []);
  if (!moves.length) {
    topCandidates = [];
    updateCandidateListUI();
    return;
  }

  // 2) board stats
  const oldBoard = game.board;
  const oldMaxH  = boardMaxHeight(oldBoard);
  const oldHoles = countHoles(oldBoard);

  // 3) simulate each move & capture post‑clear
  let sumFlow = 0, maxFlow = 0;
  moves.forEach(m => {
    // a) raw flow
    const sk = game.get_state_key();
    let f;
    if (nnModel && nnModel.loaded) {
      const vec = new Float32Array(encodeStateAction(oldBoard, game.current_piece, m.action_key));
      f = Math.exp(Math.max(-20, Math.min(20, nnModel.forward(vec))));
    } else {
      agent._ensure_action_exists(sk, m.action_key);
      f = Math.exp(agent.log_flows[sk][m.action_key]);
    }
    m.rawFlow = f;
    sumFlow  += f;
    maxFlow   = Math.max(maxFlow, f);

    // b) stamp piece
    let bc = deepCopy(oldBoard);
    m.piece.shape.forEach((row, r) =>
      row.forEach((v,c) => { if (v) bc[m.piece.y+r][m.piece.x+c] = 1; })
    );

    // c) clear + gravity cascade
    let totalClears = 0;
    let fullRows = bc
      .map((row,i) => row.every(v=>v===1) ? i : -1)
      .filter(i => i>=0);
    while (fullRows.length) {
      clearRowsAndDrop(bc, fullRows);
      totalClears += fullRows.length;
      fullRows = bc
        .map((row,i) => row.every(v=>v===1) ? i : -1)
        .filter(i => i>=0);
    }

    // d) metrics
    m.linesCleared = totalClears;
    m.resultHeight = boardMaxHeight(bc);
    m.heightInc    = m.resultHeight > oldMaxH;
    m.holeDelta    = countHoles(bc) - oldHoles;
    m.resultBoard  = bc;
  });

  // 4) beam‑blend
  const beamScores = moves.map(m => beamEvaluate(m));
  const minB = Math.min(...beamScores), maxB = Math.max(...beamScores);
  const bDen = (maxB - minB) || 1, α = 0.7;

  // 5) combinedScore + bonuses
  moves.forEach((m,i) => {
    // base blend
    const flowN = m.rawFlow / maxFlow;
    const beamN = (beamScores[i] - minB) / bDen;
    m.combinedScore = α*flowN + (1-α)*beamN;

    // your heuristics
    if (m.holeDelta > 0) m.combinedScore -= 1;
    if (m.heightInc)     m.combinedScore -= 1;
    if (m.holeDelta < 0) m.combinedScore += (-m.holeDelta)*0.8;
    m.combinedScore += (oldMaxH - m.resultHeight)*0.2;

    // enforce “outside vertical I/L” rule
    const w = m.piece.shape[0].length;
    const h = m.piece.shape.length;
    const isVertical = w === 1 && (m.piece.type === 'I' || m.piece.type === 'L');
    if (isVertical) {
      const col = m.piece.x;
      // only column 0 or COLS-1
      if (col === 0 || col === COLS-1) {
        // simulate where it would land
        const landY = landingRow(oldBoard, col, h);
        // ensure that *all* cells at (col, landY..landY+h-1) are empty
        let canFit = true;
        for (let dy = 0; dy < h; dy++) {
          if (oldBoard[landY + dy][col]) {
            canFit = false; break;
          }
        }
        if (canFit) {
          // huge bonus to guarantee we pick it
          m.combinedScore += 100;
        }
      }
    }
  });

  // 6) bucket by preference
  const clears     = moves.filter(m=>m.linesCleared>0);
  const holeFills  = moves.filter(m=>!m.linesCleared && m.holeDelta<0 && !m.heightInc);
  const cleanFlats = moves.filter(m=>!m.linesCleared && m.holeDelta===0 && !m.heightInc);
  let preferred = clears.length   ? clears
                : holeFills.length? holeFills
                : cleanFlats.length? cleanFlats
                : moves;

  // 7) top‑3 + backfill
  preferred.sort((a,b)=>b.combinedScore - a.combinedScore);
  const chosen = preferred.slice(0,3);
  for (let m of moves.sort((a,b)=>b.combinedScore - a.combinedScore)) {
    if (chosen.length>=3) break;
    if (!chosen.includes(m)) chosen.push(m);
  }

  // 8) annotate
  chosen.forEach(m => {
    m.flowDisplay = (m.rawFlow/maxFlow).toFixed(2);
    m.prob        = m.rawFlow / sumFlow;
    m.probDisplay = (m.prob*100).toFixed(1) + '%';
  });

  // 9) final sort
  chosen.sort((a,b)=>
    a.resultHeight - b.resultHeight ||
    b.combinedScore - a.combinedScore
  );

  // 🔟 render
  topCandidates = chosen;
  assignCandidateColors(topCandidates);
  updateCandidateListUI();
  const first = candidateListEl.querySelector('.candidate');
  if (first) first.click();
}

function updateCandidateListUI() {
  candidateListEl.innerHTML = "";

  // topCandidates is already sorted by prob descending
  topCandidates.forEach((c, i) => {
    const div = document.createElement("div");
    div.className = "candidate";
    div.style.borderLeft = `10px solid ${c.color}`;  // 0→green,1→yellow,2→red
    div.innerHTML = `
      <h3>${c.action_key}</h3>
      <p>Flow: ${c.flowDisplay}</p>
      <p>Prob: ${c.probDisplay}</p>
    `;
    div.onclick = () => doSelectCandidate(c.action_key);
    candidateListEl.appendChild(div);
  });
}


    // ====================== INIT & JSON LOADER ======================
    document.addEventListener('DOMContentLoaded', () => {
      canvas          = document.getElementById('tetrisCanvas');
      ctx             = canvas.getContext('2d');
      candidateListEl = document.getElementById('candidateList');
      resetBtn        = document.getElementById('resetBtn');
      game    = new TetrisGame();
      agent   = new TrajectoryBalanceAgent();
      nnModel = new NeuralFlowNet();
      // only load pretrained_flows_tb.json now:
      fetch('pretrained_flows_tb.json')
        .then(res => {
          if (!res.ok) throw new Error('Failed to load pretrained_flows_tb.json');
          return res.json();
        })
        .then(tbJson => {
          agent.loadFromJSON(tbJson);
          console.log('[Frontend] TB weights loaded, logZ=', tbJson.logZ);
        })
        .catch(err => {
          console.warn('[Frontend] Could not load TB weights:', err);
        })
        .finally(() => {
          setInterval(tickGame, TICK_INTERVAL);
          fetchCandidateMoves();
          resetBtn.addEventListener('click', doResetGame);
          requestAnimationFrame(animate);
        });
    });

  
  </script>
  

  <script>
  function toggleNavMenu() {
    const nav = document.getElementById('mainNav');
    nav.classList.toggle('show');
  }
  function scrollDown() {
    window.scrollBy({
      top: window.innerHeight,
      left: 0,
      behavior: 'smooth'
    });
  }
  
  /* Intersection observer to highlight sections */
  document.addEventListener("DOMContentLoaded", () => {
    const sections = document.querySelectorAll("main section");
    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting) {
          entry.target.classList.add("active");
        } else {
          entry.target.classList.remove("active");
        }
      });
    }, { threshold: 0.3 });
    sections.forEach(section => observer.observe(section));
  });
  </script>
  
  <!-- 
     (Line 1699) Particle JS config
     We'll set up the background so it looks dynamic. 
  -->
  <script>
  particlesJS("particles-js", {
    particles: {
      number: { value: 60, density: { enable: true, value_area: 800 } },
      color: { value: "#00bfff" },
      shape: {
        type: "circle",
        stroke: { width: 0, color: "#000" },
        polygon: { nb_sides: 5 }
      },
      opacity: { value: 0.5, random: true },
      size: { value: 3, random: true },
      line_linked: {
        enable: true,
        distance: 150,
        color: "#00bfff",
        opacity: 0.4,
        width: 1
      },
      move: {
        enable: true,
        speed: 2,
        direction: "none",
        random: false,
        straight: false,
        out_mode: "out"
      }
    },
    interactivity: {
      detect_on: "canvas",
      events: {
        onhover: { enable: false },
        onclick: { enable: false }
      }
    },
    retina_detect: true
  });
  </script>
  
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      if(typeof renderMathInElement==="function"){
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
          ]
        });
      }
    });
  </script>
  
  